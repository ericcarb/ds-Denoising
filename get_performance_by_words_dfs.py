'''
README

Reads a words_dfs.txt (generated by sed.py) and generates
more succinct text files of the results. (out.txt and dout.txt)

Can be buggy if noisy or denoised benchmarks have NaN accuracy or precision.
Use both out.txt and dout.txt to ensure all results are included.
'''

from ecosystem.benchmark.transcription import TranscriptionBenchmark
from ecosystem.gql_client import GQLClient
from ecosystem.job import Job, JobManager
from ecosystem.veritone_engines import *
import pandas as pd
import os
from subprocess import Popen, PIPE
from io import StringIO
from collections import defaultdict
from pprint import pprint

def read_words_dfs (filepath):
	'''
	Reads the words_dfs.txt into a list.
	'''
	if filepath.split('/')[-1] != 'words_dfs.txt':
		if filepath[-1] != '/':
			filepath += '/'
		filepath += 'words_dfs.txt'

	dfs = []
	df = ''
	asset = ''
	name = ''
	with open(filepath, 'r') as inf:
		for line in inf:
			if line == '\n':
				if len(df) > 0 and 'locomotive' in name:
					dfs.append((name, asset, df))
				df = ''
				asset = ''
			elif line[0] == '#' and line[1] == 'p':
				name_asset = line.split('#')
				name = name_asset[1]
				asset = name_asset[2]
			elif line[0] in ',0123456789':
				df += line

	return dfs

def select_dfs (all_words_dfs_filepaths, target_folders):
	'''
	Selects words_dfs.txt of interest from the output of bash's find command.

	all_words_dfs_filepaths is a list of the filepaths to words_dfs.txt
	target_folders is a list of subdirectory names that must be within the 
	  words_dfs.txt filepath for the filepath to be selected
	'''
	curr_words_dfs = []
	for fp in all_words_dfs_filepaths:
		use = False
		for folder_name in target_folders:
			if folder_name in fp.split('/'):
				use = True
		if use:
			curr_words_dfs.append(fp)

	return curr_words_dfs


def main ():
	'''
	Finds all words_dfs.txt in root_dir, then reads the dataframes and uses the
	ecosystem library to generate a more succinct text file with accuracy and precision
	'''
	root_dir = '/home/eric.carb/denoiser_benchmark/'
	cmd = ['find', root_dir, '-name', 'words_dfs.txt']
	p = Popen(cmd, stdout=PIPE, stderr=PIPE)
	stdout, stderr = p.communicate()
	all_words_dfs_filepaths = stdout.decode('utf-8').split('\n')
	#print(all_words_dfs_filepaths)
	curr_words_dfs = select_dfs(all_words_dfs_filepaths)
	#print(curr_words_dfs)
	engine_speechmatics = Engine('Speechmatics',
                             	 'Supernova-English (USA)',
                             	 'transcribe-speechmatics-container-en-us')
	engine_kaldi = Engine('Kaldi',
                      	  'Conductor Engine Kaldi',
                      	  '6ad5c1c4-1c7f-4097-99a4-e48fbc9e719b')
	errors = []
	to_write = ''
	noisy_results = defaultdict(dict) # {noise : {snr : (acc, prec)}}
	denoised_results = defaultdict(dict) # {noise : {snr : (acc, prec)}}
	for words_dfs_filepath in curr_words_dfs:
		print(words_dfs_filepath+'\n---')
		p = words_dfs_filepath.split('/')
		experiment_name = p[4]
		noise_engine = (p[5], p[6])
		if 'sm' in noise_engine[0] or 'kaldi' in noise_engine[0]:
			engine_name = noise_engine[0]
			noise_condition = noise_engine[1]
		else:
			noise_condition = noise_engine[0]
			engine_name = noise_engine[1]

		if 'kaldi' in engine_name:
			used_engine = engine_kaldi
		else:
			used_engine = engine_speechmatics
		print('exp', experiment_name)
		print('noise', noise_condition)
		print('eng', engine_name)
		words_dfs = read_words_dfs(words_dfs_filepath)
		for name, asset, words_df in words_dfs:
			all_jobs = [Job(used_engine)]
			all_engine_names = [job.engine.name for job in all_jobs]
			all_words_df = pd.read_csv(StringIO(words_df), sep=',')
			try:
				perf_measures = TranscriptionBenchmark.compute_performance_measures(all_words_df, all_jobs)
				name = name.split('__')[-1][:-1]
				snr = name.split('_')[-1]
				name = '_'.join(name.split('_')[:-1])
				acc_prec = str(perf_measures).split('\n')[1].split()[1:]
				to_write += name + '\t' + snr + '\n\t' + acc_prec[0] + '\t' + acc_prec[1] + '\n'
				if 'denoised' in words_dfs_filepath:
					denoised_results[name][snr] = (acc_prec[0], acc_prec[1])
				else:
					noisy_results[name][snr] = (acc_prec[0], acc_prec[1])
			except Exception as e:
				print(e)
				errors.append(e)
	for e in errors:
		print(e)
	print('\n\n')
	#print(to_write)
	print(len(noisy_results))
	print(len(denoised_results))

	outf = open('out.txt', 'a')
	for k, v in noisy_results.items():
		dv = denoised_results[k]
		outf.write('\n'+k+'\n')
		outf.write('\t\t{}\t{}\t\t{}\n'.format('SNR', 'Acc.', 'Prec.'))
		if k == '' or list(v.items())[0][0] == '':
			continue
		for snr, ap in sorted(v.items(), key=lambda kv : int(kv[0][:-3])):
			dap = dv[snr]
			outf.write('  noisy\t\t{}\t{}\t{}\n'.format(snr, ap[0], ap[1]))
			outf.write('  denoised\t{}\t{}\t{}\n\n'.format(snr, dap[0], dap[1]))
	outf.close()

	outf = open('dout.txt', 'a')
	for k, v in denoised_results.items():
		dv = noisy_results[k]
		outf.write('\n'+k+'\n')
		outf.write('\t\t{}\t{}\t\t{}\n'.format('SNR', 'Acc.', 'Prec.'))
		if k == '' or list(v.items())[0][0] == '':
			continue
		for snr, ap in sorted(v.items(), key=lambda kv : int(kv[0][:-3])):
			try:
				dap = dv[snr]
			except Exception as e:
				dap = ('NaN', 'NaN')
			outf.write('  noisy\t\t{}\t{}\t{}\n'.format(snr, dap[0], dap[1]))
			outf.write('  denoised\t{}\t{}\t{}\n\n'.format(snr, ap[0], ap[1]))
	outf.close()



if __name__ == '__main__':
	main()
